\section{Arbres de décision améliorés}\label{chapter-ML-section-XGB}
% https://docs.google.com/presentation/d/1WTeOmpcj3Fr4KU2-ZASnBPjyNd5OddY6Bmr9YYZaXDs/edit#slide=id.g53a3150aec_0_7
La librairie
\XGBOOST~\cite{xgboost}
(\emph{eXtreme Gradient Boosting})
permet de construire des
arbres de décision améliorés.
De nombreuses compétitions Kaggle~\cite{kaggle_challenge} ont été remportées grâce à eux.
Ils présentent l'avantage d'être généralement plus rapides à entraîner que les réseaux de neurones présentés section~\ref{chapter-ML-section-DNN},
et peuvent fournir des prédictions même si une des variables d'entrée est manquante, ce qui n'est pas le cas des réseaux de neurones.
\subsection{Arbres de décision}\label{chapter-ML-section-XGB-decision_trees}
Un arbre de décision (non amélioré) est une succession de questions
dont les réponses mènent à un résultat final,
comme illustré sur la figure~\ref{fig-decision_tree_schema}.
Chaque réponse à une question crée une \og branche \fg{} menant à une nouvelle question (en bleu)
ou à une réponse finale sur une \og feuille \fg{} (en vert).
\begin{figure}[h]
\centering\small
\begin{tikzpicture}
\def\DeltaX{1.5}
\def\DeltaY{1.25}

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (A1) at (3.125*\DeltaX,0*\DeltaY) {Est-ce le weekend ?\vphantom{Àq}};

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (B1) at (1.625*\DeltaX,-1*\DeltaY) {Réunion ou cours dans l'agenda ?\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (B2) at (4.625*\DeltaX,-1*\DeltaY) {se reposer\vphantom{Àq}};

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (C1) at (.5*\DeltaX,-2*\DeltaY) {Résultats disponibles ?\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (C2) at (2.75*\DeltaX,-2*\DeltaY) {y aller\vphantom{Àq}};

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (D1) at (-2*\DeltaX,-3*\DeltaY) {Calculs en cours ?\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (D2) at (2.75*\DeltaX,-3*\DeltaY) {Retours sur le manuscrit ?\vphantom{Àq}};

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (E1) at (-3*\DeltaX,-4*\DeltaY) {les lancer\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (E2) at (-1*\DeltaX,-4*\DeltaY) {les contrôler\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (E3) at (1.5*\DeltaX,-4*\DeltaY) {Chapitre terminé ?\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (E4) at (4*\DeltaX,-4*\DeltaY) {les appliquer\vphantom{Àq}};

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (F1) at (.5*\DeltaX,-5*\DeltaY) {rédiger\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorblue1] (F2) at (2.5*\DeltaX,-5*\DeltaY) {Chapitre relu ?\vphantom{Àq}};

\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (G1) at (1.5*\DeltaX,-6*\DeltaY) {relire\vphantom{Àq}};
\node [draw, thick, rectangle, rounded corners = 5pt, fill = ltcolorgreen1] (G2) at (3.5*\DeltaX,-6*\DeltaY) {envoyer pour relecture\vphantom{Àq}};

\draw [thick, -latex, ltcolorred] (A1) to (B1) ;
\draw [thick, -latex, ltcolorgreen] (A1) to (B2) ;
\draw (A1.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

\draw [thick, -latex, ltcolorred] (B1) to (C1) ;
\draw [thick, -latex, ltcolorgreen] (B1) to (C2) ;
\draw (B1.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

\draw [thick, -latex, ltcolorred] (C1) to (D1) ;
\draw [thick, -latex, ltcolorgreen] (C1) to (D2) ;
\draw (C1.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

\draw [thick, -latex, ltcolorred] (D1) to (E1) ;
\draw [thick, -latex, ltcolorgreen] (D1) to (E2) ;
\draw (D1.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

\draw [thick, -latex, ltcolorred] (D2) to (E3) ;
\draw [thick, -latex, ltcolorgreen] (D2) to (E4) ;
\draw (D2.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

\draw [thick, -latex, ltcolorred] (E3) to (F1) ;
\draw [thick, -latex, ltcolorgreen] (E3) to (F2) ;
\draw (E3.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

\draw [thick, -latex, ltcolorred] (F2) to (G1) ;
\draw [thick, -latex, ltcolorgreen] (F2) to (G2) ;
\draw (F2.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};

%\draw (A.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};
%\draw (B.south) node [below] {{\color{ltcolorgreen}oui} \qquad\qquad {\color{ltcolorred}non}};
%\draw (C.south) node [below] {\quad {\color{ltcolorred}non} {\color{ltcolorgreen}oui}};
%\draw (D.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};
%\draw (E.south) node [below] {{\color{ltcolorred}non} {\color{ltcolorgreen}oui}};
\end{tikzpicture}
\caption[Exemple d'un arbre de décision.]{Exemple d'un arbre de décision utilisable par un doctorant.}
\label{fig-decision_tree_schema}
\end{figure}
\par
De tels arbres peuvent être utilisés avec des variables numériques.
Dans ce cas, chaque question consiste en une condition sur l'une des variables, par exemple $\pT^{\mu} > \SI{50}{\GeV}$.
Le choix de la variable ($\pT^{\mu}$) et de la coupure correspondante (\SI{50}{\GeV}) à utiliser pour former deux nouvelles branches $b_1$ (condition fausse) et $b_2$ (condition vraie) se base sur la similarité $S$.
Il s'agit d'une variable quantifiant les écarts d'une liste de valeurs $y_i$ à la moyenne de celles-ci $\average{y}$.
Elle est définie comme
\begin{equation}
S = \frac{1}{N} \left(\sum_{i=1}^N r_{i} \right)^2
\msep
r_i = y_i - \frac{1}{N} \sum_{j=1}^N y_j = y_i - \average{y}
\end{equation}
où $N$ est la taille de la liste de valeurs,
$r_i$ le résiduel de $y_i$
et $y_{i}$ la $i$\up{e} valeur de $y$.
\par
Le gain $G$ obtenu par la création de deux nouvelles branches $b_1$ et $b_2$ s'exprime
\begin{equation}
G = S_{b_1} + S_{b_2} - S_{b_1+b_2}
\end{equation}
avec
$S_{b_1+b_2}$ la similarité de la liste non séparée,
$S_{b_1}$ ($S_{b_2}$) la similarité de la liste se retrouvant dans la branche $b_1$ ($b_2$).
La condition retenue pour former les deux branches est celle présentant le gain le plus élevé.
Cela revient à définir deux sous-listes dans lesquelles les valeurs de $y$ sont proches les unes des autres.
Ce processus est alors itéré sur chacune des nouvelles branches, jusqu'à ce que:
\begin{itemize}
\item le gain soit inférieur à une valeur $\gamma$ fixée;
\item la profondeur de l'arbre soit égale à une valeur \MaxDepth\ fixée;
\item la quantité d'échantillons dans une branche soit inférieure à une valeur \MinChildWeight\ fixée.
\end{itemize}
Les paramètres $\gamma$, \MaxDepth\ et \MinChildWeight, choisis par l'utilisateur, sont des hyper-paramètres.
%Par défaut, $\gamma=0$ et $\MinChildWeight=1$.
\subsection{\emph{Gradient Boosting} et descente de gradient}\label{chapter-ML-section-XGB-grad_boost}
La technique du \emph{Gradient Boosting} est
l'utilisation de modèles simples,
ici des arbres de décision,
pour obtenir un modèle global plus robuste.
La construction se fait de manière itérative.
\par
La première étape consiste à créer un arbre de décision, noté $M_0$, comme exposé dans la section~\ref{chapter-ML-section-XGB-decision_trees}.
La fonction associée à ce modèle est $F_0$.
Puis à chaque étape $k\geq1$,
un arbre de décision $M_k$ est construit avec pour objectif de prédire,
pour une entrée $\vec{x}_i$,
\begin{equation}
\ytruei - F_{k-1}(\vec{x}_i)
\end{equation}
avec
\ytruei\ la valeur que doit prédire le modèle global %pour l'entrée $\vec{x}_i$
et
$F_{k-1}$ la fonction du modèle issu de l'étape ${k-1}$.
Le modèle $M_k$ corrige donc l'écart résiduel des prédictions $\set{\ypredi}$ du modèle $F_{k-1}$ à $\set{\ytruei}$.
Les prédictions $F_k$ du modèle global s'expriment donc
\begin{equation}
\ypredi
=
F_k(\vec{x}_i)
=
F_{k-1}(\vec{x}_i) + \eta\,M_k(\vec{x}_i)
=
M_0(\vec{x}_i) + \eta \sum_{l=1}^{k} M_l(\vec{x}_i)
\end{equation}
avec
$\eta$ le taux d'apprentissage, inférieur à 1, permettant de corriger progressivement l'écart résiduel.
L'itération s'arrête lorsque le nombre maximal d'estimateurs \Nestimators\ est atteint.
Les grandeurs $\eta$ et \Nestimators\ sont également des hyper-paramètres.
Le modèle global obtenu est ici un arbre de décision amélioré.
%\emph{The Elements of statistical learning} : Trees have one aspect that prevents them from bering the ideal tool for predictive learning, namely inaccuracy. --> they work great with the data used to create them, but they are not flexible when it comes to classifying new samples...
\par
La dérivée partielle de \LossMSE\
par rapport à \ypredi\
s'exprime
\begin{equation}
\pdv{\LossMSE(\ytruei, \ypredi)}{\ypredi} = \ypredi - \ytruei
\mend
\end{equation}
Ainsi, la cible de $M_k$ définie précédemment est
\begin{equation}
\ytruei - F_{k-1}(\vec{x}_i)
=
- \pdv{\LossMSE(\ytruei, F_{k-1}(\vec{x}_i))}{F_{k-1}(\vec{x}_i)}
\mend[,]
\end{equation}
ce qui revient à appliquer la descente de gradient avec $\Loss=\LossMSE$.
À partir de ce constat,
il est possible de généraliser le \emph{Gradient Boosting}
en considérant que la cible de $M_k$ est
\begin{equation}
- \pdv{\Loss(\ytruei, F_{k-1}(\vec{x}_i))}{F_{k-1}(\vec{x}_i)}
=
- \grad_{F_{k-1}(\vec{x}_i)} \left(\Loss(\ytruei, F_{k-1}(\vec{x}_i))\right)
\end{equation}
avec \Loss\ une fonction de coût quelconque.
\par
Une itération de l'entraînement consiste ainsi en l'ajout d'un estimateur au modèle.
Un arrêt prématuré est réalisé lorsque l'erreur quadratique moyenne (\LossMSE) ne diminue pas sur le jeu de validation pendant 5 itérations.