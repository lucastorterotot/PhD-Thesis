\section{Arbres de décision basés sur \XGBOOST}\label{chapter-ML-section-XGB}


% https://docs.google.com/presentation/d/1WTeOmpcj3Fr4KU2-ZASnBPjyNd5OddY6Bmr9YYZaXDs/edit#slide=id.g53a3150aec_0_7


why? faster a good at challenges (see Colin's slides from \emph{somewhere in the past})

\emph{The Elements of statistical learning} : Trees have one aspect that prevents them from bering the ideal tool for predictive learning, namely inaccuracy.
--> they work great with the data used to create them, but they are not flexible when it comes to classifying new samples...
\subsection{Principe}
tree

max depth, n estimators

\subsection{Entraînement}
\subsubsection{Amélioration des prédictions}

objective

learning rate

early stopping

\includegraphics[width=.45\textwidth]{\PhDthesisdir/plots_and_images/my_plots/ML/from_ML_plots/global_comparisons/XGB_structures-full_mape.pdf}



\subsubsection{Surentraînement}

\input{\PhDthesisdir/plots_and_images/my_plots/ML/overfitting_and_early_stopping/examples-pyplot.pgf}


