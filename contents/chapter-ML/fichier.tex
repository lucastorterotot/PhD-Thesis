\chapter{Reconstruction de la masse d'une résonance grâce au \emph{Machine Learning}}\label{chapter-ML}


\remarque{Citations incontournables:
\begin{itemize}
\item \DELPHES~3.4.2~\cite{Delphes,Delphes_additions}?
\item CMS Fast Simulation (\FASTSIM)~\cite{FastSim_2011,FastSim_2014,FastSim_2017_1,FastSim_2017_2}
\item \PYTHIA~8.235~\cite{pythia8.2}
\item \FASTJET~\cite{Cacciari:2011ma,Cacciari:2006} % Fast Jet
\item \KERAS~\cite{keras}
\item \TENSORFLOW~\cite{tensorflow}
\item \XGB~\cite{xgboost}
\item for an example of nn use in HEP\\
\fullcite{jet_flavor_deep_nn}
\item \fullcite{Sarle1994NeuralNA}
\item \fullcite{BARTSCHI201929}
\item \SVFIT~\cite{SVFit_Bianchini_2014}
\end{itemize}}

Citer également la thèse de Gaël:\\\fullcite{Gael_thesis}

et celle de Mortiz:\\\fullcite{scham_moritz_2020_21993}

et celle de Tanja:\\\fullcite{kopf_tanja_2019_21500}

\begin{itemize}
\item type of samples/events
\item preselection (small HTT analysis)
\item inputs
\item performances: métrique?
\item mass range + plots
\item METcov + plots
\item PU + plots
\end{itemize}

Follow report from 2021-02-04 but for section 3 : We saw that predictions come out too low, which already is a motivation to put larger weights on higher masses, i.e. to weight by truth. Choosing sqrt(truth) is of course just a guess then

\remarque{\begin{itemize}
\item $\phi$ and $[2\pi]$: how would the DNN handle this?
\item low mass: low stats and boundaries
\item high mass: reco? train only on high mass and see if still have this effect, then change loss function?
\item additionnal info from the event? FSR? Additionnal photons?
\item add terms for symmetry
\item compare with SVFit !!
\end{itemize}}

\section*{Étapes des choix}
\subsection*{Inputs variables}
phéno, tau1 tau2 MET pT eta phi + mT 1 2 tt tot.
\subsection*{Inputs events}
sélection des événements, target flattening. pas de PU

80-800 GeV (aller plus loin que \cite{BARTSCHI201929})
\subsection*{DNN}

\fullcite{DNN}


Comme dans~\cite{BARTSCHI201929}, table 1.

output activation function to linear instead of relu to not cut?

changement de la structure: 3 couches de 1000 neurones?

loss mse

optimizer adadelta

w\_init\_mode uniform

early stopping

Changement du mass range (biais aux bords) -> down to 50 GeV


\begin{code}{python}
tf.keras.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
\end{code}

parameters to optimize = weights and biais

%Glorot :  X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks”, in Proceedings of the thirteenth international conference on artificial intelligence and statistics, p. 249. 2010.


\subsection*{XGB}
avoid overfitting, max\_depth

learning rate (eta)

num\_round, early stopping (choose value of 5)

loss

\input{\PhDthesisdir/contents/chapter-ML/introduction/fichier_introduction.tex}

\section{Le \emph{Machine Learning}}
\subsection{Généralités}
\subsection{Le \emph{Gradient Boosting}}
\subsection{Le \emph{Deep Learning}}

\section{Application du \emph{Machine Learning} aux événements $\Higgs\to\tau\tau$}
\subsection{Génération des événements}
\subsection{Variables d'entrées}
\subsection{Performances sur les événements de test}
\subsection{Performances sur les événements de l'analyse CMS}

\section{Prise en compte de l'empilement}
\subsection{Génération des événements}
\subsection{Performances} (sur ces nouveaux événements)
\subsection{Variables d'entrées supplémentaires}
\subsection{Performances} (avec les nouvelles variables)

\section{Effets sur les résultats de l'analyse MSSM HTT}
(remplacement de mttot par les prédictions du meilleur modèle, nouveaux plots d'exclusion, comparaison)

\input{\PhDthesisdir/contents/chapter-ML/conclusion/fichier_conclusion.tex}