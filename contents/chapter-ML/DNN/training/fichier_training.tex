\subsection{Entraînement}\label{chapter-ML-section-DNN-training}
L'entraînement d'un NN est le réglage des paramètres des neurones du réseau situés sur les couches cachées et la couche de sortie.
Il s'agit des poids $w_i$ et du biais $b$.
Pour un DNN avec
$n_\text{in} = \num{27}$ variables d'entrée,
$\NLayers = \num{3}$ couches cachées
de $\NNeurons = \num{1000}$ neurones,
le nombre de paramètres est ainsi de
\begin{align}
N_\text{params.}
&= \underbrace{\NNeurons \times (n_\text{in} + 1)}_\text{couche cachée 1} \!\!\!&\!\!\!+\,\,\,& \underbrace{(\NLayers -1)\times \NNeurons \times(\NNeurons+1)}_\text{autres couches cachées} \!\!\!&\!\!\!+\,\,\,& \underbrace{\NNeurons +1\vphantom{()}}_\text{couche de sortie}
\nonumber\\&
=
\num{28000} \!\!\!&\!\!\!+\,\,\,& 2\times\num{1001000} \!\!\!&\!\!\!+\,\,\,& \num{1001}
=
\num{2031001}
\mend[,]
\end{align}
soit près de deux millions.
Les termes \og $+1$ \fg{} correspondent aux biais $b$ à ajouter au nombre d'entrées des neurones.
\subsubsection{Initialisation des paramètres}
Les biais $b$ sont initialement fixés à 0,
les poids $w_i$ à une valeur constante donnée ou aléatoirement selon une loi de probabilité.
Le mode d'initialisation est un hyper-paramètre du modèle.
Lors de ces travaux, nous avons testé les lois normale et uniforme.
Dans le cas des DNNs, ces modes d'initialisation peuvent être améliorés par la méthode de \citeauthor{glorot}~\cite{glorot} afin de faciliter l'entraînement.
Il s'agit alors des lois \og Glorot uniforme \fg{} et \og Glorot normale \fg, également testées.
\subsubsection{Optimisation des paramètres}\label{chapter-ML-section-DNN-training-optimizers}
L'optimisation des paramètres est réalisée
en utilisant les mini-lots introduits en section~\ref{chapter-ML-section-gradient_descent}.
Un maximum de \num{500} époques est autorisé,
avec un arrêt prématuré au bout de \num{20} époques sans diminution de l'erreur absolue moyenne (\LossMAE) sur les données de validation.
%\par
Plusieurs algorithmes d'optimisation existent~\cite{DNN}, présentés de manière non exhaustive ci-après.
%Le premier, BGD~\cite{SGD}, est introduit section~\ref{chapter-ML-section-gradient_descent}.
%Cependant, le choix d'une valeur optimale du taux d'apprentissage $\eta$ est ardu.
%Or, les modèles y sont très sensibles~\cite{DNN}.
%C'est pourquoi d'autres algorithmes d'optimisation ont été développés.
\paragraph{\emph{Batch Gradient Descent} (BGD)} \cite{SGD}
L'algorithme BGD,
déjà introduit en section~\ref{chapter-ML-section-gradient_descent},
applique la méthode de descente de gradient
sur les mini-lots.
Le bruit dû à la composition aléatoire des mini-lots
permet de s'échapper des minimums locaux
mais dégrade la précision une fois au minimum global.
Pour palier cet effet, le taux d'apprentissage $\eta$ peut être diminué à chaque époque.
La condition sur les taux d'apprentissage $\eta_k$ avec $k$ l'époque afin de s'assurer de la convergence du modèle optimisé par BGD est~\cite{DNN}
\begin{equation}
\sum_{k=1}^\infty \eta_k = \infty
\msep
\sum_{k=1}^\infty \eta_k^2 < \infty
\mend
\end{equation}
La mise à jour des paramètres à la fin d'un mini-lot pendant l'époque $k$ est alors réalisée selon
\begin{equation}
p \to p - \eta_k \average{\grad(\Loss)}_\text{mini-lot} \cdot \bvec_p = p - \eta_k \Average{\pdv{\Loss}{p}}_\text{mini-lot}
\mend
\end{equation}
\paragraph{BGD avec moments} \cite{DNN}
Les moments sont une \og mémoire \fg{} des valeurs du gradient de la fonction de coût des époques précédentes.
Ce peut être vu comme une inertie du mouvement du modèle dans l'espace des paramètres,
prise en compte à travers une vitesse $\vec{v}$ définie initialement par l'utilisateur et mise à jour à chaque mini-lot selon
\begin{align}
\vec{v}[t-1] \to \vec{v}[t]
&=
\alpha\vec{v}[t-1] - \eta_k \average{\grad(\Loss)[t]}_\text{mini-lot}
\\
\Rightarrow
\vec{v}[t]\cdot\bvec_p = v_p[t]
&=
\alpha v_p[t-1] - \eta_k \Average{\pdv{\Loss}{p} [t]}_\text{mini-lot}
\end{align}
avec
$t$ l'indice d'itération ou indice temporel de l'entraînement,
et
$0\leq\alpha<1$ le paramètre des moments.
La mise à jour des paramètres lors de l'itération $t$ se fait alors selon
\begin{equation}
p [t-1] \to p[t]
=
p[t-1] + v_p[t]
=
p[t-1] + \alpha v_p[t-1] - \eta_k \Average{\pdv{\Loss}{p} [t]}_\text{mini-lot}
\mend
\end{equation}
\paragraph{\emph{Adaptive Gradient} (AdaGrad)} \cite{adagrad}
L'algorithme AdaGrad adapte le taux d'apprentissage individuellement pour chaque paramètre $p$
à l'aide d'une
variable de mémoire $\vec{r}$.
Elle est initialement définie à $\vec{0}$ et est modifiée à chaque mini-lot selon
\begin{equation}
\vec{r}\cdot\bvec_p = r_p \to r_p + \Average{\pdv{\Loss}{p}}_\text{mini-lot}^2
\label{eq-AdaGrad_memory}
\mend
\end{equation}
La mise à jour des paramètres se fait alors suivant
\begin{equation}
p \to p - \eta \frac{1}{\sqrt{r_p}+\delta} \Average{\pdv{\Loss}{p}}_\text{mini-lot}
\end{equation}
où $\delta$ est une variable de régularisation évitant les divisions par zéro.
Le taux d'apprentissage effectif pour le paramètre $p$
est ainsi $\eta$ divisé par la somme quadratique des gradients précédents $\sqrt{r_p}$.
\par
Plus un paramètre modifie la valeur de la fonction de coût, plus sa modification est progressive.
Dans l'optique de la recherche d'un minimum, cela revient à descendre une pente lentement et à se mouvoir rapidement dans une direction plane.
Cependant, l'accumulation depuis le début de l'entraînement des gradients au carré dans $r_p$ peut mener à une diminution excessive du taux d'apprentissage effectif d'un paramètre.
\paragraph{RMSProp} \cite{RMSProp}
L'algorithme RMSProp consiste en une légère modification de AdaGrad.
Une décroissance exponentielle de la mémoire des gradients passés est mise en place en remplaçant~\eqref{eq-AdaGrad_memory} par
\begin{equation}
r_p \to \rho \, r_p + (1-\rho)\Average{\pdv{\Loss}{p}}_\text{mini-lot}^2
\end{equation}
où $0<\rho<1$ est le taux de diminution de la mémoire.
RMSProp est ainsi une version de AdaGrad dont la mémoire est plus adaptée à la situation locale.
\paragraph{\emph{Adaptive Delta} (AdaDelta)}
À l'instar de RMSProp, AdaDelta est une modification de AdaGrad visant à améliorer l'effet de mémoire.
La variable $r_p$ est mise à jour par~\eqref{eq-AdaGrad_memory}.
Cependant, la valeur précédente de $r_p$ est également utilisée lors de la mise à jour de $p$.
Ainsi, lors de l'itération $t$,
\begin{equation}
p [t-1] \to p [t] = p[t-1] - \frac{\sqrt{r_p[t-1]}+\delta}{\sqrt{r_p[t]}+\delta} \Average{\pdv{\Loss}{p} [t]}_\text{mini-lot}
\mend
\end{equation}
Il n'y a donc pas besoin de définir un taux d'apprentissage initial avec AdaDelta.
\paragraph{\emph{Adaptive Moments} (Adam)} \cite{adam,DNN}
L'algorithme Adam
est une combinaison
de la méthode des moments
et
de RMSProp.
Il adapte donc le taux d'apprentissage pour chaque paramètre à chaque mini-lot.
Pour cela sont définis initialement:
\begin{itemize}
\item le pas $\epsilon=\num{0.001}$;
\item les moments d'ordres 1 et 2, $\vec{v}=\vec{0}$ et $\vec{r}=\vec{0}$;
\item les taux de diminution de moments d'ordre 1 et 2, $\rho_1=\num{0.9}$ et $\rho_2=\num{0.999}$;
\item le paramètre temporel $t=0$.
\end{itemize}
Puis, à chaque mini-lot, les moments sont redéfinis selon
\begin{equation}
\vec{v}\cdot\bvec_p =
v_p \to \rho_1 v_p + (1-\rho_1) \Average{\pdv{\Loss}{p}}_\text{mini-lot}
\msep
\vec{r}\cdot\bvec_p =
r_p \to \rho_2 r_p + (1-\rho_2) \Average{\pdv{\Loss}{p}}_\text{mini-lot}^2
\mend
\end{equation}
Le biais d'initialisation des moments est corrigé en appliquant
\begin{equation}
t \to t+1
\msep
v_p \to \frac{v_p}{1-\rho_1^t}
\msep
r_p \to \frac{r_p}{1-\rho_2^t}
\mend
\end{equation}
Les paramètres du modèle sont alors mis à jour selon
\begin{equation}
p \to p - \epsilon \frac{v_p}{\sqrt{r_p}+\delta}
\end{equation}
où
$\delta=\num{e-8}$ permet de stabiliser les calculs en évitant une division par zéro.
%most of them = backpropagation
%\par
%local minima?
%\par
%backpropagation and vanishing grad