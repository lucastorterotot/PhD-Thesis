\subsection{Neurones}\label{chapter-ML-section-DNN-neuron}
\subsubsection{Principe}
Un neurone est une entité ayant
un certain nombre d'entrées $x_j$, $j\in\set{1,\ldots,n}$,
auxquelles sont associées des poids $w_j$,
un biais $b$
et
une fonction $f$ dite d'\og activation \fg, discutée section~\ref{chapter-ML-section-DNN-neuron-activ_fct}.
Les poids $w_j$ et le biais $b$ sont les paramètres du neurone,
la fonction d'activation est un hyper-paramètre.
La sortie $s$ du neurone s'exprime comme
\begin{equation}
s = f\left(\sum_{j=1}^n w_jx_j + b\right)
\mend
\end{equation}
Le fonctionnement d'un neurone est résumé sur la figure~\ref{fig-chapter-ML-section-DNN-neuron-neuron_structure}.
\begin{figure}[h]
\centering
\input{\PhDthesisdir/plots_and_images/my_plots/ML/neuron/neuron_fr.tex}
\caption[Structure d'un neurone.]{Structure d'un neurone. Une fonction $f$ dite d'\og activation \fg{} est appliquée à la somme des entrées $x_j$ pondérées par les poids $w_j$ et du biais $b$ afin d'obtenir la valeur de sortie.}
\label{fig-chapter-ML-section-DNN-neuron-neuron_structure}
\end{figure}
\subsubsection{Fonctions d'activation}\label{chapter-ML-section-DNN-neuron-activ_fct}
En principe, toute fonction définie sur l'ensemble d'existence de chacune des entrées $x_j$ peut être utilisée comme fonction d'activation.
Elles sont ainsi définies sur $\mathbb{R}$.
Les plus utilisées sont:
\begin{figure}[p]
\centering

\subcaptionbox{Tangente hyperbolique.\label{subfig-act_fct-tanh}}[.45\textwidth]
{\input{\PhDthesisdir/plots_and_images/my_plots/ML/activation_functions/tanh-pyplot.pgf}\vspace{-\baselineskip}}
\hfill
\subcaptionbox{ReLU.\label{subfig-act_fct-relu}}[.45\textwidth]
{\input{\PhDthesisdir/plots_and_images/my_plots/ML/activation_functions/relu-pyplot.pgf}\vspace{-\baselineskip}}

\vspace{.5\baselineskip}

\subcaptionbox{Sigmoïde.\label{subfig-act_fct-sigmoid}}[.45\textwidth]
{\input{\PhDthesisdir/plots_and_images/my_plots/ML/activation_functions/sigmoid-pyplot.pgf}\vspace{-\baselineskip}}
\hfill
\subcaptionbox{Softplus.\label{subfig-act_fct-Spl}}[.45\textwidth]
{\input{\PhDthesisdir/plots_and_images/my_plots/ML/activation_functions/softplus-pyplot.pgf}\vspace{-\baselineskip}}

\vspace{.5\baselineskip}

\subcaptionbox{Softsign.\label{subfig-act_fct-Ssg}}[.45\textwidth]
{\input{\PhDthesisdir/plots_and_images/my_plots/ML/activation_functions/softsign-pyplot.pgf}\vspace{-\baselineskip}}
\hfill
\subcaptionbox{ELU.\label{subfig-act_fct-elu}}[.45\textwidth]
{\input{\PhDthesisdir/plots_and_images/my_plots/ML/activation_functions/elu-pyplot.pgf}\vspace{-\baselineskip}}

\vspace{.5\baselineskip}

\caption[Exemples de fonctions d'activation.]{Exemples de fonctions d'activation. À gauche, des fonctions à valeurs bornées, généralement utilisées en classification. À droite, des fonctions à valeurs non bornées, utilisables pour des tâches de régression.}
\label{fig-act_fct}
\end{figure}
\begin{description}
\item[tangente hyperbolique] notée $\tanh$, définie par
\begin{equation}
\tanh
:x\mapsto
\frac{\eexp{x}-\eexp{-x}}{\eexp{x}+\eexp{-x}}
\label{eq-act_fct-tanh}
\mend[;]
\end{equation}
\item[sigmoïde] notée $\mathrm{sig}$, définie par
\begin{equation}
\mathrm{sig}
:x\mapsto
\frac{1}{1+\eexp{-x}}
\label{eq-act_fct-sig}
\mend[;]
\end{equation}
\item[Softsign] notée $\mathrm{Ssg}$, définie par
\begin{equation}
\mathrm{Ssg}
:x\mapsto
\frac{x}{1+\abs{x}}
\label{eq-act_fct-ssg}
\mend[;]
\end{equation}
\item[ReLU] (\emph{Rectified Linear Unit}), définie par
\begin{equation}
\mathrm{ReLU}
:x\mapsto
\left\lbrace \begin{aligned}
x\msep & x > 0\\
0\msep & x \leq 0
\end{aligned} \right.
\label{eq-act_fct-relu}
\mend[;]
\end{equation}
\item[Softplus] notée $\mathrm{Spl}$, définie par
\begin{equation}
\mathrm{Spl}
:x\mapsto
\ln(1+\eexp{x})
\label{eq-act_fct-spl}
\mend[;]
\end{equation}
\item[ELU] (\emph{Exponential Linear Unit}), définie par
\begin{equation}
\mathrm{ELU}
:x\mapsto
\left\lbrace \begin{aligned}
x\msep & x > 0\\
\alpha(\eexp{x}-1)\msep & x \leq 0
\end{aligned} \right.
\msep
\alpha=1
\label{eq-act_fct-elu}
\mend[;]
\end{equation}
\item[SELU] (\emph{Scaled Exponential Linear Unit}), définie par
\begin{equation}
\mathrm{SELU}
:x\mapsto
\lambda\times\left\lbrace \begin{aligned}
x\msep & x > 0\\
\alpha(\eexp{x}-1)\msep & x \leq 0
\end{aligned} \right.
\msep
\alpha \simeq \num{1.67}
\msep
\lambda \simeq \num{1.05}
\label{eq-act_fct-selu}
\mend[;]
\end{equation}
\end{description}
ou encore la fonction linéaire identité $\Id:x\mapsto x$.
Certaines d'entre elles sont représentées sur la figure~\ref{fig-act_fct}.