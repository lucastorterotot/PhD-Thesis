\subsection{Neurones}\label{chapter-ML-section-DNN-neuron}

\begin{figure}[h]
\centering
\input{\PhDthesisdir/plots_and_images/my_plots/ML/neuron/neuron_fr.tex}
\caption[Structure d'un neurone.]{Structure d'un neurone. Une fonction $f$ dite d'\og activation \fg{} est appliquée à la somme des entrées $x_i$ pondérées par les poids $w_i$ et du biais $b$ afin d'obtenir la valeur de sortie.}
\label{fig-chapter-ML-section-DNN-neuron-neuron_structure}
\end{figure}



Activation functions:
tanh, sigmoïd mostly for classification,
linear, relu, elu, selu, softmax, softplus ...